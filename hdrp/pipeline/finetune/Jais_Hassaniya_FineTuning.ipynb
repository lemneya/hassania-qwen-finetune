{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jais Fine-Tuning for Hassaniya Dialect\n",
    "\n",
    "This notebook fine-tunes the Jais Arabic LLM on Hassaniya dialect data using QLoRA.\n",
    "\n",
    "**Requirements:**\n",
    "- GPU with 10GB+ VRAM (A100, RTX 4090, A10)\n",
    "- Hugging Face account with Jais model access\n",
    "\n",
    "**Estimated Time:** 2-4 hours for Jais-13b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers accelerate datasets\n",
    "!pip install -q peft bitsandbytes\n",
    "!pip install -q sentencepiece protobuf scipy einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/lemneya/hassania-qwen-finetune.git\n",
    "%cd hassania-qwen-finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training data\n",
    "!wc -l hdrp/data/processed/exports/jais/jais_train.jsonl\n",
    "!head -1 hdrp/data/processed/exports/jais/jais_train.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hugging Face Login\n",
    "\n",
    "You need to:\n",
    "1. Create a Hugging Face account at https://huggingface.co\n",
    "2. Accept the Jais model license at https://huggingface.co/inceptionai/jais-13b-chat\n",
    "3. Create an access token at https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Option 1: Login with token\n",
    "# login(token=\"your_token_here\")\n",
    "\n",
    "# Option 2: Interactive login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Configuration\n",
    "MODEL_ID = \"inceptionai/jais-13b-chat\"  # or \"core42/jais-30b-chat-v3\"\n",
    "OUTPUT_DIR = \"models/jais-hassaniya\"\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading model with 4-bit quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Jais-formatted training data\n",
    "dataset = load_dataset(\n",
    "    'json', \n",
    "    data_files='hdrp/data/processed/exports/jais/jais_train.jsonl', \n",
    "    split='train'\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(dataset)} training examples\")\n",
    "print(f\"Sample: {dataset[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "\n",
    "# Split\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "print(f\"Train: {len(split_dataset['train'])}, Eval: {len(split_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_dataset['train'],\n",
    "    eval_dataset=split_dataset['test'],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "print(f\"This will take approximately 2-4 hours for Jais-13b\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model(f\"{OUTPUT_DIR}/final\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final\")\n",
    "print(f\"Model saved to {OUTPUT_DIR}/final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"كيف نقول 'مرحبا، كيف حالك؟' بالحسانية؟\",\n",
    "    \"Translate 'I want to buy a car' to Hassaniya dialect.\",\n",
    "    \"اشحالك؟\",\n",
    "    \"كيف أسأل عن سعر شيء في السوق بالحسانية؟\",\n",
    "]\n",
    "\n",
    "SYSTEM_PROMPT = \"اسمك مساعد حسانية، متخصص في اللهجة الحسانية الموريتانية.\"\n",
    "\n",
    "def format_prompt(user_msg):\n",
    "    return f\"\"\"### Instruction: {SYSTEM_PROMPT}\n",
    "أكمل المحادثة أدناه بين [|Human|] و [|AI|]:\n",
    "### Input: [|Human|] {user_msg}\n",
    "### Response: [|AI|]\"\"\"\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nUser: {prompt}\")\n",
    "    formatted = format_prompt(prompt)\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.split(\"### Response: [|AI|]\")[-1].strip()\n",
    "    print(f\"Jais: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download Model\n",
    "\n",
    "Run this to create a downloadable zip file of your fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r jais-hassaniya-model.zip models/jais-hassaniya/final\n",
    "print(\"Model zipped! Download jais-hassaniya-model.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
